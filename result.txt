阶段一：
我采取了李沐的代码框架，手写了一个resnet18的模型（没有任何预训练），并使用pytorch的框架在本地4060显卡上进行训练，得到了一个baseline的模型
我调整了以下内容：
1. 数据增强（图片强化）
- 目的：增加数据多样性，提高模型泛化能力
- 基础增强：
  * RandomResizedCrop: 随机裁剪并缩放到260x260
  * RandomHorizontalFlip: 水平翻转 (p=0.5)
  * RandomVerticalFlip: 垂直翻转 (p=0.5)
  * ColorJitter: 颜色抖动
    - brightness=0.2 (亮度)
    - contrast=0.2 (对比度)
    - saturation=0.2 (饱和度)
- 高级增强：
  * GaussNoise: 高斯噪声 (var_limit=(10.0, 50.0))
  * GaussianBlur: 高斯模糊 (blur_limit=(3, 7))
  * GridDistortion: 网格变形
  * OpticalDistortion: 光学变形
- 实践建议：
  * 根据数据特点选择合适的增强方法
  * 增强强度要适中，避免破坏图像语义
  * 验证集只使用基础的中心裁剪和标准化

2. 卷积层大小设计
- 原则：从浅层到深层，特征图尺寸逐渐减小，通道数逐渐增加
- 具体设置：
  * 输入层: 3通道, 260x260
  * 第一层: 64通道, kernel_size=7, stride=2
  * 中间层: 128->256->512通道
  * 最后层: 1024通道
- 关键考虑：
  * 感受野大小要足够覆盖关键特征
  * 计算量和内存占用要平衡
  * 特征图尺寸变化要合理

3. 残差层数量
- 架构：ConvNeXt Base
- 残差块分布：
  * Stage 1: 3个残差块
  * Stage 2: 3个残差块
  * Stage 3: 9个残差块
  * Stage 4: 3个残差块
- 设计原理：
  * 中间层需要更多残差块处理复杂特征
  * 浅层和深层保持较少残差块以控制计算量
- 优化建议：
  * 根据数据复杂度调整残差块数量
  * 关注skip connection的梯度流动
  * 监控各阶段的特征质量

4. 参数初始化方法
- 卷积层：Kaiming初始化
  * 原理：考虑ReLU激活函数的非线性特性
  * 方差计算：var = 2.0/fan_in
  * 适用：所有使用ReLU的卷积层
- 批归一化层：
  * weight: N(1, 0.02)
  * bias: 0
- 线性层：Xavier初始化
  * 原理：保持输入输出方差一致
  * 范围：[-1/sqrt(in_features), 1/sqrt(in_features)]
- 实践要点：
  * 预训练模型时保持原始初始化
  * 新增层需要特别注意初始化方法
  * 监控训练初期的梯度规模

5. Dropout的使用与设置
- 位置：
  * 全连接层之间：rate=0.5
  * 卷积层之后：rate=0.1
  * 最后分类器前：rate=0.3
- 训练策略：
  * 训练时开启
  * 验证/推理时关闭
  * 使用Monte Carlo Dropout进行不确定性估计
- 最佳实践：
  * 较深的网络降低dropout率
  * 配合BatchNorm使用时需要注意顺序
  * 监控验证集性能调整dropout率

6. 优化器选择与配置
- AdamW优化器：
  * 学习率：1e-3
  * 权重衰减：0.05
  * β1=0.9, β2=0.999
  * eps=1e-8
- 优势：
  * 自适应学习率调整
  * 动量和RMSprop的结合
  * 权重衰减分离
- 使用建议：
  * 预训练模型微调时降低学习率
  * 监控梯度范数确保训练稳定
  * 根据loss曲线调整超参数

7. 学习率调度策略
- 余弦退火调度：
  * 初始学习率：1e-3
  * 最小学习率：1e-6
  * 周期：单调递减
- 预热策略：
  * 预热轮数：2
  * 方式：线性增加
  * 起始学习率：1e-6
- 调度公式：
  lr = lr_min + 0.5*(lr_max-lr_min)*(1+cos(epoch/max_epochs*π))
- 实施建议：
  * 大批量训练需要更大的初始学习率
  * 预热阶段避免梯度震荡
  * 根据验证集表现调整学习率变化速度

其中影响最显著的是dropout的使用，dropout的使用可以有效防止过拟合，提高模型的泛化能力。
很快就会过拟合，训练集的准确率甚至可以达到了100%，但是测试集精确度很低
询问助教之后没有找到解决方法

阶段二：
如果不采用预训练模型，效果不好。
我采用了比赛官方的代码框架，使用预训练模型，调用了timm库中的efficientnet_b2模型，并使用pytorch的框架在kaggle的GPU上进行训练
我主要尝试了resnet18,resnet50,efficientnet_b0,efficientnet_b2,convnext_tiny,convnext_base
其中的确有很多我没有想到的内容
代码模块化，使用了cfg类封装所有配置信息，使用get_transforms函数设置数据增强，调用了torch的dataloader函数进行数据加载等等，逻辑非常清晰
1：数据处理
- 训练集增强:
  * Resize(260x260)
  * 随机翻转
  * 高斯噪声
  * 模糊处理
  * Normalize
  * CoarseDropout
- 验证/测试集:
  * 仅Resize和Normalize
2.训练策略
- 5折交叉验证 (StratifiedKFold)
  * n_splits=5
  * shuffle=True
  * random_state=42
- 训练轮次：1 epoch
- 批次大小：64

3. 推理阶段优化 (TTA)
- 使用6种测试时增强:
  * 原始图像
  * 水平翻转
  * 垂直翻转
  * 水平+垂直翻转
  * 水平翻转+水平垂直翻转
  * 垂直翻转+水平垂直翻转
- 预测结果平均:
  outputs = (outputs1 + outputs2 + outputs3 + outputs4 + outputs5 + outputs6) / 6
这些都增加了模型的性能和泛化能力，防止过拟合，但是没有考虑到数据不平衡的问题
但是我始终无法跑通代码，在kaggle上无法使用

阶段三：
通过添加日志和错误信息，我发现了先前代码的问题（动态类型出错）。我采用了新的代码框架，并使用pytorch的框架在本地4060显卡上进行训练
这次我主要使用了efficientnet_b0,efficientnet_b2,convnext_base
我主要关注了数据不均衡的优化问题，也同时添加了其他优化手段


过采样（Oversampling）：增加少数类样本的数量
我在直接把少于20的样本类别添加到20个

欠采样（Undersampling）：减少多数类样本的数量（没有采用）

WeightedRandomSampler
根据类别频率的倒数为每个样本分配采样权重
自动平衡各类别的采样频率
每个epoch都能看到足够的少数类样本，不需要实际复制数据

动态类别权重
在训练过程中动态调整类别权重
初期：强调少数类平衡
中期：逐渐过渡
后期：接近原始分布

加权交叉熵损失：
为不同类别分配不同的损失权重，基于类别频率的倒数
直接在损失层面处理类别不平衡

